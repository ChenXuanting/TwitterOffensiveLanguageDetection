{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (matthews_corrcoef, confusion_matrix, \n",
    "                              accuracy_score, f1_score, precision_score, recall_score)\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                                  AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer,\n",
    "                                  XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer,\n",
    "                                  AdamW, get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd63721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import SubtaskAData, convert_examples_to_features, SubtaskAProcessor\n",
    "\"\"\"For subtask B,C, import\n",
    "   SubtaskBData, SubtaskCData\n",
    "   SubtaskBProcessor, SubtaskCProcessor,\n",
    "   \"\"\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c217eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load train & test data\"\"\"\n",
    "\n",
    "def load_data(raw_train, raw_test):    \n",
    "    train = []\n",
    "    for i, t in enumerate(raw_train.texts):\n",
    "        if i == -1:\n",
    "            break\n",
    "        train.append((t, raw_train.labels[i]))\n",
    "    \n",
    "    test = []\n",
    "    for i, t in enumerate(raw_test.texts):\n",
    "        test.append((t, raw_test.ids[i]))\n",
    "\n",
    "    return train, test\n",
    "\n",
    "raw_train, raw_test = SubtaskAData(path=\"datasets/OffensEval20\").getData()\n",
    "#For subtask B,C, use SubtaskBProcessor, SubtaskCProcessor\n",
    "X_train, X_dev = load_data(raw_train, raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57953b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dir: cached-results/albert/subtask_A/albert-base-v2/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"!!!Modify this cell to change models!!!\"\"\"\n",
    "\n",
    "Model_type = 'albert'     #All types: bert, roberta, albert, xlmroberta\n",
    "Model_name = 'albert-base-v2'\n",
    "    #All models: 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1', 'albert-xxlarge-v1' \n",
    "                #'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2'\n",
    "                #'roberta-base', 'roberta-large'\n",
    "                #Browse https://huggingface.co/ for more pretrained models\n",
    "Task_name = 'subtask_A'   #All tasks: subtask_A, subtask_B, subtask_C\n",
    "Output_path = 'cached-results/' + Model_type + '/'\n",
    "\n",
    "seq_len = 128\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "epochs = 6\n",
    "weight_decay = 0\n",
    "LR = 5e-6\n",
    "adam_eps = 1e-9\n",
    "max_norm = 1.0\n",
    "\n",
    "Output_path = Output_path+Task_name+\"/\"+Model_name+\"/\"\n",
    "print(\"Output Dir:\",Output_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_class, model_class, tokenizer_class = AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer\n",
    "\"\"\"For bert, use BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "   roberta, use RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "   albert, use AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer,\n",
    "   xlmroberta, use XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer.\n",
    "\"\"\"  \n",
    "\n",
    "config = config_class.from_pretrained(Model_name, num_labels=2, finetuning_task=Task_name) #Task A\n",
    "tokenizer = tokenizer_class.from_pretrained((Model_name))\n",
    "\n",
    "processor = SubtaskAProcessor(X_train, X_dev)\n",
    "#For subtask B,C, use SubtaskBProcessor, SubtaskCProcessor\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b04d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(task, tokenizer):\n",
    "    \"\"\"Process training data\"\"\"\n",
    "    \n",
    "    print(\"Creating features from datasets...\")\n",
    "    label_list = processor.get_labels()\n",
    "    examples = processor.get_train_examples()\n",
    "    \n",
    "    features = convert_examples_to_features(examples, label_list, seq_len, tokenizer,\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        cls_token_segment_id= 0,\n",
    "        pad_token_segment_id= 0)\n",
    "        \n",
    "        \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n",
    "                                        \n",
    "def train(train_dataset, model, tokenizer):\n",
    "    \"\"\"Training process\"\"\"\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=adam_eps)\n",
    "        \n",
    "    print(\"===== Training =====\")\n",
    "    print(\"  Num examples =\", len(train_dataset))\n",
    "    print(\"  Num Epochs =\", epochs)\n",
    "    print(\"  Total train batch size =\", train_batch_size)\n",
    "\n",
    "    step = 0\n",
    "    tr_loss = 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(epochs, desc=\"Epoch\")\n",
    "    \n",
    "    epoch_i = 0\n",
    "    max_metric = 0\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        epoch_i += 1\n",
    "        print(\"Training Epoch\", epoch_i)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if Model_type in ['bert'] else None,\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            print(\"\\r%f\" % loss, end='')\n",
    "\n",
    "                \n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Save model checkpoint\n",
    "        output_dir = os.path.join(Output_path, 'checkpoint-{}'.format(epoch_i))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        model.save_pretrained(output_dir)\n",
    "        print(\" \")\n",
    "        print(\"Saving model checkpoint to\", output_dir)\n",
    "\n",
    "\n",
    "    return step, tr_loss / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199928a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa65ea1405c5433c9b026bc76a87afac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training =====\n",
      "  Num examples = 14100\n",
      "  Num Epochs = 6\n",
      "  Total train batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a13d3b44744f0a9038c4058b0ab489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b4f189d5774337a1ca7749a2de7f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n",
      "0.796114 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd9a56bdd948b4ae8b408bcca91107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2\n",
      "0.422683 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47126aea3a5847caad9e9b202ee79d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3\n",
      "1.313888 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a489966037f740cc8f342a8cead71b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4\n",
      "0.942182 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec3a89d70dc47f0af9715f4130d8673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5\n",
      "0.002344 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad44355a8ac4b29ba17513ee5d279ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6\n",
      "0.000175 \n",
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/checkpoint-6\n",
      " step = 3525 , average loss = 3.1543332351691333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Do train\"\"\"\n",
    "model = model_class.from_pretrained(Model_name, num_labels=num_labels)\n",
    "model.to(device)\n",
    "train_dataset = load_examples(Task_name, tokenizer)\n",
    "step, tr_loss = train(train_dataset, model, tokenizer)\n",
    "print(\" step =\", step, \", average loss =\", tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71bd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to cached-results/albert/subtask_A/albert-base-v2/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cached-results/albert/subtask_A/albert-base-v2/tokenizer_config.json',\n",
       " 'cached-results/albert/subtask_A/albert-base-v2/special_tokens_map.json',\n",
       " 'cached-results/albert/subtask_A/albert-base-v2/spiece.model',\n",
       " 'cached-results/albert/subtask_A/albert-base-v2/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Save trained model\"\"\"\n",
    "if not os.path.exists(Output_path):\n",
    "        os.makedirs(Output_path)\n",
    "print(\"Saving model checkpoint to \" + Output_path)\n",
    "\n",
    "model.save_pretrained(Output_path)\n",
    "tokenizer.save_pretrained(Output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff9b952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ee24c5d6424fa49092d9c148c37758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22fd4444d7d4d8c91211b7ad52278af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b2201245e94f5db6a1b128c55ed5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a6dca2ed5d412497be6959f0cad7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2abdedca70c43e8a97a4c98cdc7937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8af0749fb644ba88a740cf1580da1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0627a4fb765d4be6b440a3895b749e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adc31cf72ae4b339411198fc470ceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ce687d975466cb9f766b22b74685d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1d71c1e2de4844b1918e6c5e45769b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\\checkpoint-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d80852606e24f299bf8694da4199132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f827391fe69461ab2728c21a00eb27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached-results/albert/subtask_A/albert-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4e85f7e3fe4665b690a1962a01a4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 3887\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec055351a34d09a3eaf6df470eb9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_prediction(task, X_predict, tokenizer):\n",
    "    \"\"\"Process testing data\"\"\"\n",
    "    \n",
    "    processor = SubtaskAProcessor(X_predict, None)\n",
    "    #For subtask B,C, use SubtaskBProcessor, SubtaskCProcessor\n",
    "    examples = processor.get_train_examples()\n",
    "    features = convert_examples_to_features(examples, label_list, seq_len, tokenizer,\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        cls_token_segment_id= 0,\n",
    "        pad_token_segment_id= 0)\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n",
    "\n",
    "def predict_sentences(sentences):\n",
    "    \"\"\"Testing process\"\"\"\n",
    "    \n",
    "    X = [(s, 'OFF') for s in sentences]\n",
    "    predict_dataset = prepare_prediction(Task_name, X, tokenizer)\n",
    "    eval_sampler = torch.utils.data.SequentialSampler(predict_dataset)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(predict_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    prefix = \"\"\n",
    "\n",
    "    print(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    print(\"  Num examples =\", len(predict_dataset))\n",
    "    print(\"  Batch size =\", eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if Model_type in ['bert'] else None,\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    probabilities = sm(torch.from_numpy(preds)).numpy()\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def eval_stats(labels, preds):\n",
    "    \"\"\"Generate stats\"\"\"\n",
    "    \n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    # macro\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    p = precision_score(labels, preds, average='macro')\n",
    "    r = recall_score(labels, preds, average='macro')\n",
    "    \n",
    "    # not\n",
    "    f1_0 = f1_score(labels, preds, average='binary', pos_label=0)\n",
    "    p_0 = precision_score(labels, preds, average='binary', pos_label=0)\n",
    "    r_0 = recall_score(labels, preds, average='binary', pos_label=0)\n",
    "    \n",
    "    # off\n",
    "    f1_1 = f1_score(labels, preds, average='binary', pos_label=1)\n",
    "    p_1 = precision_score(labels, preds, average='binary', pos_label=1)\n",
    "    r_1 = recall_score(labels, preds, average='binary', pos_label=1)\n",
    "    \n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"acc\" : acc,\n",
    "        \"f1\" : f1,\n",
    "        \"precision\" : p,\n",
    "        \"recall\" : r,\n",
    "        \"p_not\" : p_0,\n",
    "        \"r_not\" : r_0,\n",
    "        \"f1_not\" : f1_0,\n",
    "        \"p_off\" : p_1,\n",
    "        \"r_off\" : r_1,\n",
    "        \"f1_off\" : f1_1\n",
    "    }, get_mismatched(labels, preds)\n",
    "\n",
    "\"\"\"Do test\"\"\"\n",
    "test_sentences, test_ids = zip(*X_dev)\n",
    "fold_model_dirs = list(os.path.dirname(c) for c in sorted(glob.glob(Output_path + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "emsemble_preds = np.empty((len(test_sentences), len(fold_model_dirs)))\n",
    "for i, fold_dir in enumerate(fold_model_dirs):\n",
    "    print(fold_dir)\n",
    "    model = model_class.from_pretrained(fold_dir)\n",
    "    model.to(device)\n",
    "    prob_scores = predict_sentences(test_sentences)\n",
    "    predicted_labels = [a.argmax() for a in prob_scores]\n",
    "    emsemble_preds[:, i] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf338d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions: 1\n",
      "cached-results/albert/subtask_A/albert-base-v2\\testset_predictions.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save predictions\"\"\"\n",
    "\n",
    "pickle.dump(emsemble_preds, file=open(os.path.join(Output_path, \"testset_predictions.p\"), \"wb\"))\n",
    "mean_preds = emsemble_preds.mean(axis=1)\n",
    "mean_preds.tolist()\n",
    "\n",
    "path = 'cached-results/albert/subtask_A/'\n",
    "    #For subtask B,C, use subtask_B, subtask_C\n",
    "files = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.p' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "print(\"Total number of predictions:\",len(files))\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc03d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT:\tP: 98.501\tR: 91.27\tF1: 94.75\n",
      "OFF:\tP: 80.95\tR: 96.39\tF1: 88.0\n",
      "F1: 91.37\tACC: 92.69\n"
     ]
    }
   ],
   "source": [
    "def get_mismatched(labels, preds):\n",
    "    mismatched = labels != preds\n",
    "    examples = processor.get_dev_examples()\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "    \n",
    "    return wrong\n",
    "\n",
    "def taskA_getid(label):\n",
    "    if label == 'NOT':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def taskB_getid(label):\n",
    "    if label == 'TIN':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test_sentences, test_ids = zip(*X_dev)\n",
    "\n",
    "\n",
    "\"\"\"Emsemble preds from different models\"\"\"\n",
    "preds = []\n",
    "for f in files:\n",
    "    #if 'albert-xlarge-v2' in f:\n",
    "        #preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    if 'albert-xlarge-v1' in f:\n",
    "        preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    if 'albert-xxlarge-v2' in f:\n",
    "        preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    if 'albert-xxlarge-v1' in f:\n",
    "        preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    if 'albert-base-v2' in f:\n",
    "        preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    #if 'albert-base-v1' in f:\n",
    "        #preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    #if 'roberta-base' in f:\n",
    "        #preds.append(pickle.load(open( f, \"rb\" )))\n",
    "    #if 'albert-large-v2' in f:\n",
    "        #preds.append(pickle.load(open( f, \"rb\" )))\n",
    "        \n",
    "merged_preds = np.concatenate(preds, axis = 1)\n",
    "\n",
    "majority_preds = []\n",
    "for i in range(merged_preds.shape[0]):\n",
    "    majority_preds.append(Counter(merged_preds[i].astype(int)).most_common(1)[0][0])\n",
    "mean_preds = merged_preds.mean(axis=1)\n",
    "final_preds = majority_preds\n",
    "\n",
    "lables = []\n",
    "for i, t in enumerate(raw_test.texts):\n",
    "      lables.append(taskA_getid(raw_test.labels[i]))\n",
    "    #For subtask B,C, use taskB_getid, taskC_getid\n",
    "        \n",
    "result, wrong = eval_stats(np.array(lables), final_preds)\n",
    "print(\"NOT:\" + \"\\t\" +  \"P: %s\" %(str(round(result[\"p_not\"]*100, 3))) + \"\\t\" +  \"R: %s\" %(str(round(result[\"r_not\"]*100, 2))) + \"\\t\" +  \"F1: %s\" %(str(round(result[\"f1_not\"]*100, 2))))\n",
    "print(\"OFF:\" + \"\\t\" +  \"P: %s\" %(str(round(result[\"p_off\"]*100, 2))) + \"\\t\" +  \"R: %s\" %(str(round(result[\"r_off\"]*100, 2))) + \"\\t\" +  \"F1: %s\" %(str(round(result[\"f1_off\"]*100, 2))))\n",
    "print(\"F1: %s\" %(str(round(result[\"f1\"]*100, 2))) + \"\\t\" + \"ACC: %s\" %(str(round(result[\"acc\"]*100, 2))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
